'''
前提条件：
词表长度为：21128
词向量长度为：768
输入文本长度为：4

起始输入矩阵形状（4，）文本长度的一维向量
###############################################################################################
一、embedding层概况
1、word embedding层（普通的embedding层）：词表长度 * 词向量长度 （21128 * 768）
2、sentence embedding层（判断上下文关系层）：2 * 词向量长度（2 * 768）
3、position embedding层（判断位置信息层）：文本长度 * 词向量长度（4 * 768）
4、归一化层 (768,)  词向量长度的1维向量
经过embedding层后，输入维度变为
（4， 768）：文本长度 * 词向量长度
###############################################################################################
二、self-attention层概况
1、多头机制
将embedding层输出分为12个矩阵
即：（12 * 4 * 64）：头数 * 文本长度 * 词向量长度/头数（768/12=64）

2、分别经过Q、K、V，3个线性层（理论上线性层应为：y = wx + b，为了方便理解，暂忽略b的存在，忽略b不影响矩阵形状）
Q层：文本长度 * 词向量长度/头数 （64 * 64）
经过Q层，embedd层的输入，变为：
（4 * 64）：文本长度 * 词向量长度/头数
K层：词向量长度/头数 * 词向量长度/头数 （64 * 64）
经过K层，embedd层的输入，变为：
（4 * 64）：文本长度 * 词向量长度/头数
2.1、计算词与词的注意力
2.1.1、将经过K层的结果进行阵列转置（T）
即：转置后的K层输出结果为（64 * 4）：词向量长度/头数 * 文本长度
2.1.2、将经过K层并转置后的结果与Q层结果相乘
即：(4 * 64) * (64 * 4) = (4 * 4)：文本长度 * 文本长度
2.1.3、sqrt（dk）与逐行softmax归一化处理
dk：词向量长度/头数（经验数值）
矩阵中每个数字先除sqrt(dk)再softmax,其目的为：防止经过softmax后，每行只有一个数接近1，其余数接近0
输出结果为：(4 * 4)：文本长度 * 文本长度
V层：词向量长度/头数 * 词向量长度/头数 （64 * 64）
即：（4 * 4） * （64 * 64） = （4 * 64）：文本长度 * 词向量长度

3、将所有矩阵拼接
结果为：（4 * 768） 文本长度 * 词向量长度

4、线性层 词向量长度 * 词向量长度 （768 * 768）
结果为：（4 * 768） 文本长度 * 词向量长度
###############################################################################################
三、add & normalize层
1、残差机制
将矩阵的拼接结果[二-4]与embedding输出结果[一-4]进行相加
目的：保留原始数据
结果为：（4 * 768） 文本长度 * 词向量长度
2、归一化层(768,)  词向量长度的1维向量
结果为：（4 * 768） 文本长度 * 词向量长度
###############################################################################################
四、feed forward层
1、gelu激活层
结果为：（4 * 768） 文本长度 * 词向量长度
2、feed forward层（共2个线性层，先扩大4倍后缩小4倍）
目的：增加可训练参数
2.1、第一个线性层 词向量长度 * （词向量长度 * 4） （768 * 3072）
结果为：（4 * 3072） 文本长度 * （词向量长度 * 4）
2.2、第二个线性层 （词向量长度 * 4） * 词向量长度 （3072 * 768）
结果为：（4 * 768） 文本长度 * 词向量长度
3、残差机制
将[三-2]结果与[四-2.2]结果相加
结果为：（4 * 768） 文本长度 * 词向量长度
4、归一化层 (768,)  词向量长度的1维向量
结果为：（4 * 768） 文本长度 * 词向量长度
###############################################################################################
五、12层的transform
将[二]到[四]的过程循环12次就是完整的bert模型
###############################################################################################
总结：在不考虑线性层b的情况下
该模型循环1次的训练参数量为：Q + K + V + QKV后的线性层 + 归一化层 + 第一层feed forward层 + 第二层feed forward层 + 归一化层
即：768 * 768 *3 + 768 * 768  + 768 + 768 * 3072 + 3072 * 768 + 768 = 7079424
     （Q、K、V）  （QKV后线性层）（归一化）   （两层的feed forward）  （归一化）
'''