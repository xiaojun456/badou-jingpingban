{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96084467-12cb-45be-8239-d8e91a3bc93f",
   "metadata": {},
   "source": [
    "bert模型结构、各部分形状、权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36792245-4d13-4db2-861c-21bc4b4bfe07",
   "metadata": {},
   "source": [
    "输入数据形状[batch, sen_len]，词表长度vocab_len\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82f21632-7c37-4971-8469-08319ec8de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据输入的形状是：1 * 4\n",
      "word_embedding：weight的形状是：21128 * 768\n",
      "数据经过word_embedding之后的形状是：4 * 768\n",
      "position_embedding:weight的形状是：512 * 768\n",
      "数据经过position_embedding之后的形状是：4 * 768\n",
      "token_type_embedding:weight的形状是：2 * 768\n",
      "数据经过token_type_embedding之后的形状是：4 * 768\n",
      "三个embedding层都经过之后数据的形状是：4 * 768\n",
      "embedding后的归一化层weight的形状是：768,\n",
      "embedding后的归一化层bias的形状是：768,\n",
      "数据经过embedding后归一化层之后的形状是：4 * 768\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "输入数据形状[batch, sen_len]，词表长度vocab_len，隐空间维度hidden_size，模型能够处理的最大序列长度max_position_embeddings，序列类型数量type_vocab_size\n",
    "\"\"\"\n",
    "# embedding\n",
    "batch = 1\n",
    "sen_len = 4\n",
    "hidden_size = 768\n",
    "vocab_len = 21128\n",
    "max_position_embeddings = 512\n",
    "type_vocab_size = 2\n",
    "print(f\"数据输入的形状是：{batch} * {sen_len}\")\n",
    "print(f\"word_embedding：weight的形状是：{vocab_len} * {hidden_size}\")\n",
    "print(f\"数据经过word_embedding之后的形状是：{sen_len} * {hidden_size}\")\n",
    "print(f\"position_embedding:weight的形状是：{max_position_embeddings} * {hidden_size}\")\n",
    "print(f\"数据经过position_embedding之后的形状是：{sen_len} * {hidden_size}\")\n",
    "print(f\"token_type_embedding:weight的形状是：{type_vocab_size} * {hidden_size}\")\n",
    "print(f\"数据经过token_type_embedding之后的形状是：{sen_len} * {hidden_size}\")\n",
    "print(f\"三个embedding层都经过之后数据的形状是：{sen_len} * {hidden_size}\")\n",
    "print(f\"embedding后的归一化层weight的形状是：{hidden_size},\")\n",
    "print(f\"embedding后的归一化层bias的形状是：{hidden_size},\")\n",
    "print(f\"数据经过embedding后归一化层之后的形状是：{sen_len} * {hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc8eb05b-12e0-4062-b2c5-a385c5038b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_w的形状是：768 * 768\n",
      "q_b的形状是：768,\n",
      "k_w的形状是：768 * 768\n",
      "k_b的形状是：768,\n",
      "v_w的形状是：768 * 768\n",
      "v_b的形状是：768,\n",
      "经过x*w.T + b的计算后，qkv的形状均为：4 * 768\n",
      "qkv转化为多头时的形状为：4 * 12 * 64\n",
      "为了每个头进行计算，qkv转化的形状均为：12 * 4 * 64\n",
      "q * k.T的形状为：12 * 4 * 4\n",
      "q * k.T / d的形状为：12 * 4 * 4\n",
      "softmax(qk.T/d)的形状为：12 * 4 * 4\n",
      "softmax(qk.T/d)v的形状为：12 * 4 * 64\n",
      "多头机制转换成原数据维度的形状为:4 * 768\n",
      "attention输出层weight的形状为:768 * 768\n",
      "attention输出层bias的形状为:768,\n",
      "数据经过attention输出层后的形状为：4 * 768\n",
      "经过embedding后的原数据和经过attention层输出的数据相加后的形状为：4 * 768\n",
      "attention的归一化层的weight的形状是：768,\n",
      "attention的归一化层的bias的形状是：768,\n",
      "经过attention的归一化层后x的形状为：4 * 768\n",
      "前馈传播中间层intermediate_weight的形状是：3072 * 768\n",
      "前馈传播中间层intermediate_bias的形状是：3072,\n",
      "x经过前馈传播中间层(x * w.T + b)之后的形状是:4 * 3072\n",
      "x经过gelu激活函数后的形状是：4 * 3072\n",
      "前馈传播输出层output_weight的形状是：768 * 3072\n",
      "前馈传播输出层output_bias的形状是：768,\n",
      "x经过前馈传播输出层(x * w.T + b)之后的形状是：4 * 768\n",
      "前馈传播前的x和经过前馈传播后的x相加后的形状为：4 * 768\n",
      "最终的归一化层的weight的形状是：768,\n",
      "最终稿的归一化层的bias的形状是：768,\n",
      "transformer层最后的归一化层后x的形状为：4 * 768\n"
     ]
    }
   ],
   "source": [
    "# transformer\n",
    "# self_attention\n",
    "\"\"\"\n",
    "多头注意力机制头数num_attention_heads\n",
    "\"\"\"\n",
    "num_attention_heads = 12\n",
    "print(f\"q_w的形状是：{hidden_size} * {hidden_size}\")\n",
    "print(f\"q_b的形状是：{hidden_size},\")\n",
    "print(f\"k_w的形状是：{hidden_size} * {hidden_size}\")\n",
    "print(f\"k_b的形状是：{hidden_size},\")\n",
    "print(f\"v_w的形状是：{hidden_size} * {hidden_size}\")\n",
    "print(f\"v_b的形状是：{hidden_size},\")\n",
    "print(f\"经过x*w.T + b的计算后，qkv的形状均为：{sen_len} * {hidden_size}\")\n",
    "# multi-heads\n",
    "attention_head_size = int(hidden_size / num_attention_heads)\n",
    "print(f\"qkv转化为多头时的形状为：{sen_len} * {num_attention_heads} * {attention_head_size}\")\n",
    "print(f\"为了每个头进行计算，qkv转化的形状均为：{num_attention_heads} * {sen_len} * {attention_head_size}\")\n",
    "# attention计算：attention(q,k,v)=softmax(qk.T/d)v\n",
    "print(f\"q * k.T的形状为：{num_attention_heads} * {sen_len} * {sen_len}\")\n",
    "print(f\"q * k.T / d的形状为：{num_attention_heads} * {sen_len} * {sen_len}\")\n",
    "print(f\"softmax(qk.T/d)的形状为：{num_attention_heads} * {sen_len} * {sen_len}\")\n",
    "print(f\"softmax(qk.T/d)v的形状为：{num_attention_heads} * {sen_len} * {attention_head_size}\")\n",
    "print(f\"多头机制转换成原数据维度的形状为:{sen_len} * {hidden_size}\")\n",
    "print(f\"attention输出层weight的形状为:{hidden_size} * {hidden_size}\")\n",
    "print(f\"attention输出层bias的形状为:{hidden_size},\")\n",
    "print(f\"数据经过attention输出层后的形状为：{sen_len} * {hidden_size}\")\n",
    "# batch normalization + residual connection\n",
    "print(f\"经过embedding后的原数据和经过attention层输出的数据相加后的形状为：{sen_len} * {hidden_size}\")\n",
    "print(f\"attention的归一化层的weight的形状是：{hidden_size},\")\n",
    "print(f\"attention的归一化层的bias的形状是：{hidden_size},\")\n",
    "print(f\"经过attention的归一化层后x的形状为：{sen_len} * {hidden_size}\")\n",
    "# feed forward\n",
    "\"\"\"\n",
    "前馈传播中间层维度intermediate_size\n",
    "\"\"\"\n",
    "intermediate_size = 3072\n",
    "print(f\"前馈传播中间层intermediate_weight的形状是：{intermediate_size} * {hidden_size}\")\n",
    "print(f\"前馈传播中间层intermediate_bias的形状是：{intermediate_size},\")\n",
    "print(f\"x经过前馈传播中间层(x * w.T + b)之后的形状是:{sen_len} * {intermediate_size}\")\n",
    "print(f\"x经过gelu激活函数后的形状是：{sen_len} * {intermediate_size}\")\n",
    "print(f\"前馈传播输出层output_weight的形状是：{hidden_size} * {intermediate_size}\")\n",
    "print(f\"前馈传播输出层output_bias的形状是：{hidden_size},\")\n",
    "print(f\"x经过前馈传播输出层(x * w.T + b)之后的形状是：{sen_len} * {hidden_size}\")\n",
    "# batch normalization + residual connection\n",
    "print(f\"前馈传播前的x和经过前馈传播后的x相加后的形状为：{sen_len} * {hidden_size}\")\n",
    "print(f\"最终的归一化层的weight的形状是：{hidden_size},\")\n",
    "print(f\"最终稿的归一化层的bias的形状是：{hidden_size},\")\n",
    "print(f\"transformer层最后的归一化层后x的形状为：{sen_len} * {hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d32d5e4f-7338-49bd-97af-bc8630ca4fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cls]标记输出x的形状是：768\n",
      "pooler_weight的形状是:768 * 768\n",
      "pooler_bias的形状是:768,\n",
      "经过pooler层之后x的形状是：768\n"
     ]
    }
   ],
   "source": [
    "# pooler\n",
    "print(f\"[cls]标记输出x的形状是：{hidden_size}\")\n",
    "print(f\"pooler_weight的形状是:{hidden_size} * {hidden_size}\")\n",
    "print(f\"pooler_bias的形状是:{hidden_size},\")\n",
    "print(f\"经过pooler层之后x的形状是：{hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3de911-77fc-43b2-bcba-dc4ff01f8c58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
